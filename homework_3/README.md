# Проект экспериментов с полносвязными сетями (MNIST)

В этом репозитории проведены эксперименты по исследованию влияния архитектуры полносвязных нейронных сетей на качество классификации датасета MNIST.

## Структура проекта

```
homework/
├── homework_depth_experiments.py       # Эксперименты по глубине сети
├── homework_width_experiments.py       # Эксперименты по ширине сети
├── homework_regularization_experiments.py  # Эксперименты по регуляризации
├── utils/                              # Утилитарные модули
│   ├── trainer.py                     # Функции обучения и валидации
│   ├── model_utils.py                 # Определение класса MLP
│   └── visualization_utils.py         # Функции визуализации
├── data/                               # Данные MNIST (скачиваются автоматически)
├── results/                            # Результаты экспериментов
│   ├── depth_experiments/
│   ├── width_experiments/
│   └── regularization_experiments/
├── plots/                              # Графики обучения и распределения весов
└── README.md                           # Описание проекта (этот файл)
```

## Основные результаты

### Глубина сети (Depth Experiments)

```
Depth  | Train Acc | Test Acc | Time (s) | Params    | Notes
-------|-----------|----------|----------|-----------|----------------
1      | 0.9930    | 0.9783   | 129.49   | 101,770   | без регуляции  
2      | 0.9919    | 0.9755   | 134.52   | 118,282   | без регуляции  
3      | 0.9920    | 0.9787   | 135.82   | 134,794   | без регуляции  
5      | 0.9891    | 0.9750   | 139.78   | 167,818   | без регуляции  
7      | 0.9884    | 0.9779   | 143.10   | 200,842   | без регуляции  
1      | 0.9737    | 0.9778   | 134.13   | 102,026   | Dropout+BN     
2      | 0.9705    | 0.9800   | 136.26   | 118,794   | Dropout+BN     
3      | 0.9668    | 0.9785   | 141.29   | 135,562   | Dropout+BN     
5      | 0.9594    | 0.9774   | 147.67   | 169,098   | Dropout+BN     
7      | 0.9524    | 0.9768   | 155.46   | 202,634   | Dropout+BN     
```

### Ширина сети (Width Experiments)

```
Widths          | Train Acc | Test Acc | Time (s) | Params    
---------------|-----------|----------|----------|-----------
[64, 32, 16]   | 0.9873    | 0.9743   | 140.36   |  53,018   
[256,128,64]   | 0.9930    | 0.9819   | 152.14   | 242,762   
[1024,512,256] | 0.9934    | 0.9847   | 158.86   |1,462,538  
[2048,1024,512]| 0.9924    | 0.9767   | 140.83   |4,235,786  
Best Grid Config: [512,128,128], Test Acc: 0.9790
```

### Регуляризация (Regularization Experiments)

```
Method                 | Train Acc | Test Acc | Time (s) | Params  
-----------------------|-----------|----------|----------|---------
No Reg                 | 0.9925    | 0.9793   | 137.11   | 242,762
Dropout 0.1            | 0.9880    | 0.9837   | 139.47   | 242,762
Dropout 0.3            | 0.9768    | 0.9792   | 150.99   | 242,762
Dropout 0.5            | 0.9537    | 0.9739   | 153.35   | 242,762
BatchNorm              | 0.9921    | 0.9800   | 144.09   | 243,658
Dropout + BatchNorm    | 0.9728    | 0.9840   | 140.50   | 243,658
L2 Reg (wd=0.01)       | 0.9578    | 0.9583   | 139.22   | 242,762
Adaptive Dropout       | —         | 0.9717   | —        | —        
```

## Выводы

* **Глубина**: оптимальной оказалась модель с 1–3 скрытыми слоями, далее наблюдается лёгкий спад на тесте при росте глубины. Dropout+BatchNorm уменьшает переобучение, но снижает train accuracy.
* **Ширина**: широкие слои (\[1024,512,256]) дают наилучший баланс точности и числа параметров. Слишком большие сети (2048,1024,512) могут переобучаться.
* **Регуляризация**: комбинация Dropout (0.1) или BatchNorm лучше всего стабилизирует обучение. Слишком большой dropout снижает качество.
